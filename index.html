<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PAA-HOI Dataset.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAA-HOI Dataset: A Physical Attribute-Aware Human and Object Interaction Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PAA-HOI Dataset:<br> A Physical Attribute-Aware Human and Object Interaction Dataset</h1>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1MV7njj8o7oAEvtrtjfWMldjKySbnh2Et/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction.
          However, existing data sets focus mainly on details of hand grasping or object grasping preferences, often neglecting the influence of physical properties of objects on human motion. To address this limitation, we introduce the PAA-HOI Mocap dataset—the first dataset that highlights the impact of objects’ physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics.
          The dataset comprises 562 full-body interaction motion sequences performed by different gender subjects interacting with 35 3D objects in different sizes, shapes, and weights.
          This dataset stands out by significantly extending the scope of existing ones for modeling and understanding how the physical attributes of different objects influence human posture, speed, range of motion, and grasping strategies during movement interactions.
          We further demonstrate the dataset’s applicability by integrating it with several existing motion generation methods, validating its capacity to transfer realistic physical awareness.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Overview</h2>
  </div>
</section>
<figure style="width:80%; margin: 0 auto;">
  <img src="./static/pipeline.png" alt="Dataset pipeline" style="width:100%;">
  <figcaption style="margin-top: 10px; text-align: left;">
    <strong>The Overview of our dataset collection.</strong> During the motion data acquisition phase, subjects first calibrate while wearing motion capture suits, and objects require inertial sensor bias correction before performing corresponding interactive actions described in the prompts. The acquired data is then processed to fit SMPLX sequences and the 6DoF pose of objects. We adopt a unified textual template integrated with a predefined set of annotation keywords to describe HOI actions, and employ LLMs to augment the prompt for each sequence.
  </figcaption>
</figure>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Dataset Analysis</h2>
  </div>
</section>
<figure style="width:80%; margin: 0 auto;">
  <img src="./static/data_analysis.png" alt="Dataset Analysis" style="width:100%;">
  <figcaption style="margin-top: 10px; text-align: left;">
    <strong>The dataset analysis.</strong> (a) and (b) illustrate the proportional distributions of various motions and motion paths within the interaction sequences, respectively. (c) presents the changes in the number of textual descriptions before and after augmentation with different action prompts. (d) reports the average number of frames in interaction sequences involving different actions and objects of varying weights.
            </figcaption>
  </figcaption>
</figure>


<section class="section">
  <div class="container is-max-desktop">

    <!-- 并排视频标题 + 视频 -->
    <div class="columns is-centered">
      <!-- 左侧 -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4 has-text-centered">Behind The Scenes</h2>
          <video id="dollyzoom" autoplay controls muted loop playsinline style="width: 100%;">
            <source src="./static/videos/dollyzoom-stacked.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- 右侧 -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4 has-text-centered">Rendering Results</h2>
          <video id="matting-video" controls playsinline style="width: 100%;">
            <source src="./static/videos/matting.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <!-- 合并描述，与左右视频边界对齐 -->
    <div class="columns is-centered">
      <div class="column">
        <p class="has-text-justified">
          <strong>Video demonstrations:</strong> The left video showcases a Dolly zoom effect achieved through our method, which allows for camera movement through walls. The right video illustrates how our rendering process addresses the matting problem by discarding samples outside the bounding box.
        </p>
      </div>
    </div>

  </div>
</section>


    
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Behind The Scenes</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Rendering Results</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->





    
<footer class="footer" style="background-color: #f5f5f5; padding: 2rem 1rem;">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>

    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website is licensed under a 
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
          <p>
            This means you are free to borrow the 
            <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
