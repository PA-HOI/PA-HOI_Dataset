<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAA-HOI Dataset: A Physical Attribute-Aware Human and Object Interaction Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PAA-HOI Dataset:<br> A Physical Attribute-Aware Human and Object Interaction Dataset</h1>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1MV7njj8o7oAEvtrtjfWMldjKySbnh2Et/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction.
          However, existing data sets focus mainly on details of hand grasping or object grasping preferences, often neglecting the influence of physical properties of objects on human motion. To address this limitation, we introduce the PAA-HOI Mocap dataset—the first dataset that highlights the impact of objects’ physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics.
          The dataset comprises 562 full-body interaction motion sequences performed by different gender subjects interacting with 35 3D objects in different sizes, shapes, and weights.
          This dataset stands out by significantly extending the scope of existing ones for modeling and understanding how the physical attributes of different objects influence human posture, speed, range of motion, and grasping strategies during movement interactions.
          We further demonstrate the dataset’s applicability by integrating it with several existing motion generation methods, validating its capacity to transfer realistic physical awareness.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The Human-Object Interaction (HOI) task explores the dynamic interactions between humans and objects in physical environments, providing essential biomechanical and cognitive-behavioral foundations for fields such as robotics, virtual reality, and human-computer interaction.
          However, existing data sets focus mainly on details of hand grasping or object grasping preferences, often neglecting the influence of physical properties of objects on human motion. To address this limitation, we introduce the PAA-HOI Mocap dataset—the first dataset that highlights the impact of objects’ physical attributes on human motion dynamics, including human posture, moving velocity, and other motion characteristics.
          </p>

          <!-- Pipeline image with caption -->
          <figure style="text-align: center;">
            <img src="./static/pipeline.png" alt="Pipeline diagram" style="width: 100%; height: auto;">
            <figcaption style="margin-top: 0.5rem; font-style: italic; font-size: 0.9rem; text-align: left;">
              Overview of the data collection and processing pipeline for the PAA-HOI dataset.
            </figcaption>
          </figure>

          <p>
          The dataset comprises 562 full-body interaction motion sequences performed by different gender subjects interacting with 35 3D objects in different sizes, shapes, and weights.
          This dataset stands out by significantly extending the scope of existing ones for modeling and understanding how the physical attributes of different objects influence human posture, speed, range of motion, and grasping strategies during movement interactions.
          We further demonstrate the dataset’s applicability by integrating it with several existing motion generation methods, validating its capacity to transfer realistic physical awareness.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          </p>
          <!-- 添加图片和图注 -->
          <figure style="text-align: center;">
            <img src="./static/pipeline.png" alt="Pipeline diagram" style="width: 150%; height: auto;">
            <figcaption style="margin-top: 0.5rem; font-style: italic; font-size: 0.9rem; text-align: left;">
              Overview of our dataset collection. During the motion data acquisition phase, subjects first calibrate while wearing motion capture suits, and objects require inertial sensor bias correction before performing corresponding interactive actions described in the prompts. The acquired data is then processed to fit SMPLX sequences and the 6DoF pose of objects. We adopt a unified textual template integrated with a predefined set of annotation keywords to describe HOI actions, and employ a large language model (LLM) to augment the prompt for each sequence.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
    <!--/ Overview. -->
  </div>
</section>

<figure style="width:80%; margin: 0 auto; text-align: center;">
  <img src="./static/pipeline.png" alt="Dataset pipeline" style="width:100%;">
  <figcaption style="margin-top: 10px; text-align: left;">
    <strong>Overview of our dataset collection.</strong> During the motion data acquisition phase, subjects first calibrate while wearing motion capture suits, and objects require inertial sensor bias correction before performing corresponding interactive actions described in the prompts. The acquired data is then processed to fit SMPLX sequences and the 6DoF pose of objects. We adopt a unified textual template integrated with a predefined set of annotation keywords to describe HOI actions, and employ a large language model (LLM) to augment the prompt for each sequence.
  </figcaption>
</figure>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset Analysis. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset Analysis</h2>
        <div class="content has-text-justified">
          </p>
          <!-- 添加图片和图注 -->
          <figure style="text-align: center;">
            <img src="./static/data_analysis.png" alt="Pipeline diagram" style="max-width: 100%; height: auto;">
            <figcaption style="margin-top: 0.5rem; font-style: italic;">
              The dataset analysis. (a) and (b) illustrate the proportional distributions of various motions and motion paths within the interaction sequences, respectively. (c) presents the changes in the number of textual descriptions before and after augmentation with different action prompts. (d) reports the average number of frames in interaction sequences involving different actions and objects of varying weights.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
    <!--/ Dataset Analysis. -->
  </div>
</section>
    
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Behind The Scenes</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Rendering Results</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
